# 如何实现知识库问答 研究报告

## 研究日期

2025-12-29

## 研究问题

研究如何在当前代码库中实现“知识库问答”（Knowledge-base QA / RAG），包含数据注入、向量化、检索、RAG 管道与来源返回的实现细节。

## 发现摘要

- 当前 `app/knowledge` 模块已按照常见模式预留四个子模块：`ingest`（数据注入）、`retriever`（检索）、`vectorstore`（向量存储适配）、`rag_pipeline`（检索+生成的管道）。
- 目前这些模块为占位实现（stub）：`ingest` 只统计输入文档数，`retriever` 返回伪造文档，`rag_pipeline` 返回占位回答，`vectorstore` 目录未实现具体适配器。文档（`docs`）中有设计说明与配置项（如 `VECTOR_STORE`）。
- 核心下一步是选择/实现具体向量存储适配器（Chroma/FAISS/Redis 等）、嵌入与 LLM 接入点，并把 ingestion、embedding、persist 与检索流程串联起来。

## 相关文件清单

|文件路径|作用说明|关键行号|
|---|---|---|
|`app/knowledge/ingest.py`|数据注入（读取/分片/计算嵌入/写入向量库）占位实现|L1-L20|
|`app/knowledge/retriever.py`|检索策略占位（dense/sparse/hybrid 的放置点）|L1-L30|
|`app/knowledge/rag_pipeline.py`|RAG 流水线占位（调用检索并生成答案）|L1-L40|
|`app/knowledge/vectorstore/__init__.py`|向量存储适配器包占位（FAISS/Chroma/Redis 等）|L1-L40|
|`app/knowledge/__init__.py`|模块导出：`ingest, retriever, rag_pipeline, vectorstore`|L1-L10|
|`docs/2025-12-28-心青年智能体平台-项目结构.md`|项目结构与设计说明，包含向量存储与 RAG 的设计建议|L70-L120|
|`docs/tasks/01-数据库配置/2025-12-28_为项目配置MySQL数据库.md`|包含 `VECTOR_STORE` 配置示例与注意事项|L40-L260|

## 当前实现分析

总体：代码库已按常见 RAG 架构预留模块，但实际功能尚未实现。下面按阶段描述现状与预期实现。

### 核心流程（当前/预期）

当前（占位）流程：

query -> `app.knowledge.retriever.retrieve` (返回伪文档) -> `app.knowledge.rag_pipeline.answer_query` (返回占位答案和 sources)

预期（完整实现）流程：

用户查询 -> retriever 调用 VectorStore.search/top-k -> 根据检索结果构建 prompt/context -> 调用 LLM 生成回答（带引用来源与置信度）-> 返回融合后的回答与 sources

数据注入（离线/在线）流程：

原始文档(文件/DB/网页) -> `ingest` 做解析与文本拆分 -> 使用 Embedding 模型计算向量 -> `vectorstore.add_documents`（或存外部服务）-> 持久化 metadata（如 `vector_id`、原文映射）

### 关键代码片段（含文件与行号）

- `app/knowledge/ingest.py` (L1-L20)

```python
from typing import Iterable

def ingest_documents(docs: Iterable[str]):
    """Stub: accept iterable of strings and return count."""
    count = 0
    for _ in docs:
        count += 1
    return {"ingested": count}
```

- `app/knowledge/retriever.py` (L1-L30)

```python
def retrieve(query: str, top_k: int = 5) -> List[Dict]:
    """Return list of fake documents for now."""
    return [{"id": i, "text": f"Fake doc {i}", "score": 1.0} for i in range(top_k)]
```

- `app/knowledge/rag_pipeline.py` (L1-L40)

```python
from app.knowledge.retriever import retrieve

def answer_query(query: str, top_k: int = 3) -> Dict:
    docs = retrieve(query, top_k=top_k)
    response = {
        "query": query,
        "answer": "这是一个占位回答；真实实现会调用模型并附带来源",
        "sources": docs,
    }
    return response
```

- `app/knowledge/vectorstore/__init__.py` (L1-L40) 表明该包用于实现向量存储适配器接口，目前为空。

### 模块依赖关系

- `rag_pipeline` -> 依赖 `retriever`（检索）
- `retriever` -> 依赖 `vectorstore`（执行搜索）
- `ingest` -> 依赖 embedding service + `vectorstore.add_documents`

## 架构洞察

- 已有的项目文档设计明确指出应抽象 `VectorStore` 接口（`add_documents`, `search`, `persist/load`），这为后续支持多种后端（Chroma/FAISS/Redis）提供良好基础（参见 `docs/2025-12-28-心青年智能体平台-项目结构.md`）。
- 目前模块是占位实现，迁移到生产级需要明确 embedding 提供者（本地模型或云），以及 LLM 的部署/调用方式（本地小模型 or OpenAI/云 API）。
- 数据一致性建议：把大向量保存在向量服务或外部 DB，用轻量元数据表（MySQL）做映射（文档中已有相关建议）。

## 潜在风险和边缘情况

- 向量存储选择与成本：云向量 DB（如 Chroma Cloud / Pinecone）会产生成本，FAISS 本地部署在内存/磁盘管理上有复杂性。
- 隐私与敏感数据：如果知识库包含敏感信息，需要在 ingestion 阶段做敏感信息识别与过滤、访问控制、审计。
- 检索质量：仅靠 dense retrieval 可能不够，需考虑 hybrid（BM25 + dense rerank）以及 prompt 中上下文长度控制、去重与来源截断策略。

## 开放問題（需要用户确认）

1. 目标向量存储：希望优先实现哪种适配器？（Chroma / FAISS / RedisVector / Weaviate / Pinecone）
2. 嵌入模型：使用云（OpenAI embeddings），还是本地 embedding 模型（如 sentence-transformers）？
3. LLM 选择：是调用外部 API（OpenAI / Azure）还是部署本地模型？是否需要流式响应或低延迟？
4. 数据注入方式：是否需要文件、HTML、数据库多种源的自动化 ingestion？是否需要定时增量更新？
5. 安全/隐私/合规要求：是否需要在 ingestion 时做脱敏或访问控制？

## 参考資料

- `app/knowledge/ingest.py` (实现占位) — `app/knowledge/ingest.py` L1-L20
- `app/knowledge/retriever.py` (实现占位) — `app/knowledge/retriever.py` L1-L30
- `app/knowledge/rag_pipeline.py` (实现占位) — `app/knowledge/rag_pipeline.py` L1-L40
- `app/knowledge/vectorstore/__init__.py` — `app/knowledge/vectorstore/__init__.py` L1-L40
- `docs/2025-12-28-心青年智能体平台-项目结构.md` — 项目结构与 RAG 设计（L70-L120）
- `docs/tasks/01-数据库配置/2025-12-28_为项目配置MySQL数据库.md` — `VECTOR_STORE` 配置建议（L40-L260）

## 完成后

1. 文档已保存为：`docs/tasks/03-知识库问答/2025-12-29_如何实现知识库问答_研究报告.md`
2. 核心发现摘要：项目已按 RAG 模式预留模块但仍是占位实现，下一步需落地向量存储、嵌入与 LLM 接入。
3. 需要用户确认的问题（见“开放问题”部分）以推进实现优先级与技术选型。
4. 建议：如上下文占用超过 60% 请执行 `/clear` 以释放会话上下文。
