# 知识库问答 实现计划

## 计划日期

2025-12-29

## 关联研究

`docs/tasks/03-知识库问答/2025-12-29_如何实现知识库问答_研究报告.md`

## 功能概述

为现有后端实现知识库问答（Knowledge-base QA / RAG），包括：数据注入（ingest）、嵌入（embeddings）、向量存储适配（vectorstore）、检索（retriever）、以及 RAG 流水线（rag_pipeline）。首版目标是提供一个可运行的轻量原型，便于团队在现有 FastAPI 项目中快速验证并逐步替换为更完整的向量后端和模型调用。

## 技术方案摘要

- 优先实现基于 **Weaviate** 的向量存储适配器（`weaviate_adapter`），用于持久化与检索向量，支持本地或云端 Weaviate 实例。
- 嵌入与 LLM 使用 `openai` 官方库作为首选后端（需要配置 `OPENAI_API_KEY`）；同时保留 `stub` 回退实现用于离线开发与单元测试。
- 将 `ingest -> embeddings -> vectorstore.add_documents(Weaviate)` 串联，`retriever -> vectorstore.search(Weaviate)`，`rag_pipeline -> retriever + llm(OpenAI)`。
- 代码改动集中在 `app/knowledge` 目录（新增 `embeddings.py`, `llm.py`, `vectorstore/weaviate_adapter.py`；修改 `ingest.py`, `retriever.py`, `rag_pipeline.py`, `vectorstore/__init__.py`）。

---

## Phase 1: 最小可用原型（POC）

### 目标

提供一个不依赖复杂原生库（如 FAISS 编译版）的可运行原型，支持本地少量文档的注入、检索与 RAG 回答。使团队能以有限依赖（仅 numpy、requests、可选 OpenAI 客户端）完成端到端验证。


### 修改文件清单

|文件路径|修改类型|说明|
|---|---|---|
|`app/knowledge/vectorstore/__init__.py`|修改|导出 VectorStore 接口与内存实现入口|
|`app/knowledge/vectorstore/weaviate_adapter.py`|新建|实现 Weaviate 适配器（add/search/persist/load，基于 Weaviate HTTP/gRPC client）|
|`app/knowledge/embeddings.py`|新建|嵌入适配器：使用 `openai` 库为默认实现，支持 `stub` 回退|
|`app/knowledge/llm.py`|新建|LLM 适配器：使用 `openai` 库（ChatCompletion / Responses），支持 `stub` 回退|
|`app/knowledge/ingest.py`|修改|实现文本分片、调用 `embeddings` 并写入 `vectorstore`|
|`app/knowledge/retriever.py`|修改|使用 `vectorstore.search` 返回带 score 的文档列表|
|`app/knowledge/rag_pipeline.py`|修改|使用 `retriever` 构建 prompt 并调用 `llm` 生成答案，返回 sources|

### 具体变更

#### 1. 为 `vectorstore` 增加 Weaviate 适配器

文件：`app/knowledge/vectorstore/weaviate_adapter.py`

修改前（如适用）：

`（无实现）`

修改后（核心接口示例）：

```python
# app/knowledge/vectorstore/weaviate_adapter.py
from typing import List, Dict, Optional
import weaviate

class WeaviateAdapter:
    def __init__(self, url: str = 'http://localhost:8080', api_key: Optional[str] = None, index_name: str = 'Knowledge'):
        client_kwargs = {}
        if api_key:
            client_kwargs['auth_client_secret'] = weaviate.AuthApiKey(api_key)
        self.client = weaviate.Client(url=url, **client_kwargs)
        self.index_name = index_name
        # 确保类（schema）存在（简化处理）
        if not self.client.schema.contains({'class': index_name}):
            class_obj = {
                'class': index_name,
                'properties': [
                    {'name': 'text', 'dataType': ['text']},
                ]
            }
            self.client.schema.create_class(class_obj)

    def add_documents(self, docs: List[Dict], embeddings: List[List[float]]):
        with self.client.batch as batch:
            for doc, emb in zip(docs, embeddings):
                props = {k: v for k, v in doc.items() if k != 'id'}
                # 使用向量插入（Weaviate 支持外部向量）
                batch.add_data_object(props, self.index_name, vector=emb)

    def search(self, query_embedding, top_k=5):
        res = self.client.query.get(self.index_name, ['text', '_additional {id, vector, certainty}']).with_near_vector({'vector': query_embedding}).with_limit(top_k).do()
        docs = []
        for item in res.get('data', {}).get('Get', {}).get(self.index_name, []):
            meta = item.get('_additional', {})
            docs.append(dict(id=meta.get('id'), text=item.get('text'), score=meta.get('certainty'), meta=item))
        return docs

    def persist(self, path: str):
        # Weaviate 本身负责持久化，留空或实现 schema 导出
        pass

    @classmethod
    def load(cls, *args, **kwargs):
        return cls(*args, **kwargs)
```

#### 2. 新增 `embeddings` 抽象（使用 OpenAI）

文件：`app/knowledge/embeddings.py`

修改前：无

修改后（摘要）：

```python
# app/knowledge/embeddings.py
from typing import List
import os
import openai

openai.api_key = os.environ.get('OPENAI_API_KEY')

def embed_texts(texts: List[str], model: str = 'text-embedding-3-small', backend: str = 'openai') -> List[List[float]]:
    """使用 OpenAI embeddings API 返回文本向量；若 backend='stub' 则返回随机向量。"""
    if backend == 'openai':
        resp = openai.Embedding.create(model=model, input=texts)
        return [r['embedding'] for r in resp['data']]
    elif backend == 'stub':
        import numpy as np
        return [np.random.rand(1536).tolist() for _ in texts]
    else:
        raise NotImplementedError('Unsupported embed backend: ' + str(backend))
```

#### 3. 新增 `llm` 适配器（使用 OpenAI）

文件：`app/knowledge/llm.py`

修改后（摘要）：

```python
# app/knowledge/llm.py
import os
import openai
openai.api_key = os.environ.get('OPENAI_API_KEY')

def generate_answer(prompt: str, model: str = 'gpt-4o-mini', backend: str = 'openai', **kwargs) -> str:
    if backend == 'openai':
        resp = openai.ChatCompletion.create(model=model, messages=[{'role': 'user', 'content': prompt}], **kwargs)
        # 简单取第一个回复文本
        return resp['choices'][0]['message']['content']
    return '这是一个本地 stub 回答（在没有 LLM API 时使用）'
```

#### 4. 修改 `ingest.py`

文件：`app/knowledge/ingest.py`

修改前（现有 stub）：

```python
def ingest_documents(docs: Iterable[str]):
    count = 0
    for _ in docs:
        count += 1
    return {"ingested": count}
```

修改后（摘要，使用 Weaviate + OpenAI embeddings）：

```python
from typing import Iterable, Dict
from app.knowledge.embeddings import embed_texts
from app.knowledge.vectorstore.weaviate_adapter import WeaviateAdapter

def ingest_documents(docs: Iterable[Dict], vectorstore: WeaviateAdapter, embed_backend='openai'):
    texts = [d['text'] for d in docs]
    embeddings = embed_texts(texts, backend=embed_backend)
    vectorstore.add_documents(docs, embeddings)
    return {'ingested': len(texts)}
```

（包含文本分片/chunking 逻辑与 metadata 存储）

#### 5. 修改 `retriever.py` 与 `rag_pipeline.py`


文件：`app/knowledge/retriever.py`

修改后（摘要）：

```python
from app.knowledge.embeddings import embed_texts

def retrieve(query: str, vectorstore, top_k=5, embed_backend='openai'):
    q_emb = embed_texts([query], backend=embed_backend)[0]
    return vectorstore.search(q_emb, top_k=top_k)
```

文件：`app/knowledge/rag_pipeline.py`

修改后（摘要）：

```python
from app.knowledge.retriever import retrieve
from app.knowledge.llm import generate_answer

def answer_query(query: str, vectorstore, top_k=3, embed_backend='stub', llm_backend='stub'):
    docs = retrieve(query, vectorstore=vectorstore, top_k=top_k, embed_backend=embed_backend)
    context = '\n\n'.join([d['text'] for d in docs])
    prompt = f"根据以下文档回答问题：\n{context}\n问题：{query}\n请给出答案并列出来源。"
    answer = generate_answer(prompt, backend=llm_backend)
    return {"query": query, "answer": answer, "sources": docs}
```


### 成功标准

自动验证：

- [ ] `pytest tests/test_knowledge.py`（新建）通过，覆盖 ingest/retriever/rag_pipeline 的关键流程
- [ ] Weaviate 客户端能连接（默认 `http://localhost:8080` 或指定 URL），且能创建/查询 schema
- [ ] 环境变量 `OPENAI_API_KEY` 已配置并能使用 `openai` 库成功调用 embeddings + ChatCompletion（或 Responses）

手动验证：

- [ ] 用 10 条示例文档运行 `ingest_documents`（OpenAI embeddings），通过 `retriever.retrieve('示例问题')` 能返回 top-k 文档且 score 有意义
- [ ] 使用 `rag_pipeline.answer_query` 在 OpenAI 后端下返回包含 `sources` 的结构化结果

---

## Phase 2: 替换/扩展向量后端与嵌入

### 目标

支持一个真实向量存储适配器（选择性实现 Chroma 或 FAISS/Redis），并把嵌入后端从 stub/随机向量替换为真实嵌入（OpenAI embeddings 或 local sentence-transformers）。

### 修改文件清单

|文件路径|修改类型|说明|
|---|---|---|
|`app/knowledge/vectorstore/chroma.py`|新建|实现 Chroma 适配器（可选）|
|`app/knowledge/embeddings.py`|修改|加入 OpenAI 的具体实现和本地模型支持|
|`requirements.txt`|修改|添加 `chromadb` 或 `faiss-cpu`、`sentence-transformers`（按选项）|

### 成功标准

- [ ] 能用 Chroma/FAISS 及 OpenAI embeddings 将 1000 条文档持久化并检索（局部性能测试）

---

## Phase 3: 生产准备与运维（可选/后续）

### 目标

将 RAG 管道提升到可在生产环境运行的状态：持久化向量库、元数据同步到 MySQL、安全与合规、性能优化、监控与审计。

### 修改文件清单（示例）

|文件路径|修改类型|说明|
|---|---|---|
|`app/knowledge/vectorstore/redis_adapter.py`|新建|实现 Redis Vector 适配器并支持持久化| 
|`scripts/init_db.py`|修改|增加元数据表创建脚本| 
|`app/core/config.py`|修改|增加 VECTOR_STORE 配置项与密钥配置|

### 成功标准

- [ ] 在目标环境部署并能通过 `make test-e2e`（或等效脚本）完成一次完整查询流程

---

## 风险和缓解措施

|风险|可能性|影响|缓解措施|
|---|---|---|---|
|外部 API 依赖（OpenAI）缺失或费用高|高|中|提供 `stub` 与本地 `sentence-transformers` 备用实现；在文档中标注需要 API KEY 的位置及配额估算。|
|向量库安装复杂（如 faiss-cpu 编译问题）|中|高|首版采用内存实现；在可行时提供 Chroma（pip 可安装）或 Docker 化的 FAISS 服务镜像。|
|隐私敏感数据被注入知识库|中|高|在 `ingest` 阶段加入敏感信息检测与脱敏（可选），并要求 metadata 中标注权限。|

## 回滚方案

1. 所有新文件以单独 PR/commit 提交；回滚时 revert 对应 commit。
2. 向量持久化采用单独目录/文件（默认 `data/vectorstore/`），出现问题时替换为备份文件或删除并重新构建。

## 后续优化（非本次范围）

- 引入 hybrid retrieval（BM25 + dense rerank）以提升检索准确性。
- 实现 streaming 回答与逐段引用来源（对于较长答案）。
- 用向量后端的向量索引压缩与近似搜索（HNSW）提升检索效率。

## 迭代提示

每次迭代后，请回答：

1. 这个阶段划分合理吗？
2. 有没有遗漏的变更点？
3. 成功标准够不够具体？
4. 还有什么问题需要讨论？

## 完成后

1. 计划文档已保存为：`docs/tasks/03-知识库问答/2025-12-29_知识库问答_计划书.md`
2. 核心阶段：Phase 1（POC 内存实现）、Phase 2（替换为真实向量/嵌入）、Phase 3（生产准备）。
3. 由于团队成员为大一学生、对编译型依赖可能不熟，建议首轮优先内存实现与 OpenAI stub，后续再引入复杂依赖。 
4. 下一步可以让我开始 `implement_plan`：我将按 Phase 1 逐项实现并运行单元测试。是否开始？
